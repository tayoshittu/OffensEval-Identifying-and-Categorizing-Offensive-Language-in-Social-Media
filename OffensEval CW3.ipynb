{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('olid-training-v1.0.tsv', delimiter='\\t', encoding='utf-8')\n",
    "\n",
    "train_tweets = train_data[['tweet']] #Extract tweets\n",
    "train_label_a= train_data['subtask_a'].tolist() #Extract subtsak_a labels\n",
    "train_label_b= train_data['subtask_b'].tolist() #Extract subtsak_b labels\n",
    "train_label_c= train_data['subtask_c'].tolist() #Extract subtsak_c labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to clean tweets in a data frame's tweet column\n",
    "def clean_tweets(df):\n",
    "    \n",
    "    punctuations = string.punctuation\n",
    "    \n",
    "    df.loc[:, 'tweet'] = df.tweet.str.replace('@USER', '') #Remove mentions (@USER)\n",
    "    df.loc[:, 'tweet'] = df.tweet.str.replace('URL', '') #Remove URLs\n",
    "    df.loc[:, 'tweet'] = df.tweet.str.replace('&amp', 'and') #Replace ampersand (&) with and\n",
    "    df.loc[:, 'tweet'] = df.tweet.str.replace('&lt','') #Remove &lt\n",
    "    df.loc[:, 'tweet'] = df.tweet.str.replace('&gt','') #Remove &gt\n",
    "    df.loc[:, 'tweet'] = df.tweet.str.replace('\\d+','') #Remove numbers\n",
    "\n",
    "    #Remove punctuations\n",
    "    for punctuation in punctuations:\n",
    "        df.loc[:, 'tweet'] = df.tweet.str.replace(punctuation, '')\n",
    "\n",
    "    df.loc[:, 'tweet'] = df.astype(str).apply(\n",
    "        lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "    ) #Remove emojis\n",
    "    df.loc[:, 'tweet'] = df.tweet.str.strip() #Trim leading and trailing whitespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joel\\Anaconda3\\envs\\Pandas\\lib\\site-packages\\pandas\\core\\indexing.py:576: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item_labels[indexer[info_axis]]] = value\n"
     ]
    }
   ],
   "source": [
    "clean_tweets(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets_a = train_tweets['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Joel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Remove stop words\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "filtered_train_tweets_a = []\n",
    "\n",
    "for text in train_tweets_a:\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
    "    filtered_tweet = (\" \").join(tokens_without_sw)\n",
    "    filtered_train_tweets_a.append(filtered_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13240"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_train_tweets_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read tweets from test sets\n",
    "test_tweets_a=pd.read_csv('testset-levela.tsv', delimiter='\\t', encoding='utf-8', usecols=['tweet'])\n",
    "test_tweets_b=pd.read_csv('testset-levelb.tsv', delimiter='\\t', encoding='utf-8', usecols=['tweet'])\n",
    "test_tweets_c=pd.read_csv('testset-levelc.tsv', delimiter='\\t', encoding='utf-8', usecols=['tweet'])\n",
    "\n",
    "#Read tweet labels\n",
    "test_label_a=pd.read_csv('labels-levela.csv', encoding='utf-8', \n",
    "                         index_col=False, names=['id', 'class_a'], usecols=['class_a'])\n",
    "test_label_b=pd.read_csv('labels-levelb.csv', encoding='utf-8', \n",
    "                         index_col=False, names=['id', 'class_b'], usecols=['class_b'])\n",
    "test_label_c=pd.read_csv('labels-levelc.csv', encoding='utf-8', \n",
    "                         index_col=False, names=['id', 'class_c'], usecols=['class_c'])\n",
    "\n",
    "test_label_a=test_label_a['class_a'].tolist()\n",
    "test_label_b=test_label_b['class_b'].tolist()\n",
    "test_label_c=test_label_c['class_c'].tolist()\n",
    "\n",
    "#Clean tweets in test sets\n",
    "clean_tweets(test_tweets_a)\n",
    "clean_tweets(test_tweets_b)\n",
    "clean_tweets(test_tweets_c)\n",
    "\n",
    "test_tweets_a = test_tweets_a['tweet'].tolist()\n",
    "test_tweets_b = test_tweets_b['tweet'].tolist()\n",
    "test_tweets_c = test_tweets_c['tweet'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = train_tweets_a + test_tweets_a\n",
    "labels = train_label_a + test_label_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "features = vectorizer.fit_transform(tweets)\n",
    "features_nd = features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features_nd,\n",
    "    labels,\n",
    "    train_size=0.9390070921985816,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_model = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = log_model.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7965116279069767\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
